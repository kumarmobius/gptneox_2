name: LLM Benchmarks
description: Runs IFEval, BBH, MATH, GPQA (+best-effort MUSR, MMLU-Pro) on the trained model, logging COâ‚‚.
inputs:
  - name: model_py_in
    type: Data
  - name: model_config
    type: Data
  - name: model_weights
    type: Model
  - name: tokenizer_json
    type: Model
  - name: tasks
    type: String
    default: "ifeval,bbh,math,gpqa,musr,mmlu_pro"
  - name: max_new_tokens
    type: Integer
    default: "64"
  - name: batch_size
    type: Integer
    default: "1"

outputs:
  - name: benchmarks_json
    type: Data

implementation:
  container:
    image: kumar2004/kumar:v1
    command:
      - python3
      - -u
      - -c
      - |
        import os, sys, json, argparse, importlib.util, shutil, tempfile, gc
        from pathlib import Path

        def safe_write_json(path, obj):
          os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
          with open(path, "w", encoding="utf-8") as f:
            json.dump(obj, f, indent=2)

        # deferred imports with error capture (no installs here)
        torch = None
        Tokenizer = None
        EmissionsTracker = None
        LM = None
        TaskManager = None
        simple_evaluate = None
        import_errors = {}
        versions = {}

        try:
          import torch as _torch
          import torch.nn.functional as F  # noqa: F401
          torch = _torch
          versions["torch"] = torch.__version__
        except Exception as e:
          import_errors["torch"] = str(e)

        try:
          import tokenizers as _toks_mod
          from tokenizers import Tokenizer as _Tokenizer
          Tokenizer = _Tokenizer
          versions["tokenizers"] = getattr(_toks_mod, "__version__", "unknown")
        except Exception as e:
          import_errors["tokenizers"] = str(e)

        try:
          from codecarbon import EmissionsTracker as _EmissionsTracker
          EmissionsTracker = _EmissionsTracker
        except Exception as e:
          import_errors["codecarbon"] = str(e)

        try:
          from lm_eval.evaluator import simple_evaluate as _simple_evaluate
          from lm_eval.api.model import LM as _LM
          try:
            from lm_eval.tasks import TaskManager as _TaskManager
          except Exception:
            _TaskManager = None
          simple_evaluate = _simple_evaluate
          LM = _LM
          TaskManager = _TaskManager
        except Exception as e:
          import_errors["lm-eval"] = str(e)

        # avoid subclassing NoneType
        LMBase = LM if LM is not None else object

        class GPTNeoXAdapter(LMBase):
          def __init__(self, model, tokenizer, max_seq_len=512, device="cpu", batch_size=1, max_new_tokens=64):
            super().__init__()
            self.model = model
            self.tok = tokenizer
            self._device = device
            self._batch_size = batch_size
            self._max_seq_len = max_seq_len
            self._max_new_tokens = max_new_tokens

          @property
          def eot_token_id(self):
            for tok in ("<eos>", "</s>", "[EOS]", "<|endoftext|>"):
              try:
                i = self.tok.token_to_id(tok)
                if i is not None:
                  return i
              except Exception:
                pass
            return None

          def max_length(self):
            return self._max_seq_len

          def device(self):
            return self._device

          def tok_encode(self, s):
            return self.tok.encode(s).ids

          def tok_decode(self, ids):
            return self.tok.decode(ids)

          @torch.no_grad()
          def _forward_logits(self, ids):
            x = torch.tensor(ids, dtype=torch.long, device=self._device).unsqueeze(0)
            logits, _ = self.model(x)
            return logits

          @torch.no_grad()
          def loglikelihood(self, requests):
            out = []
            for context, continuation in requests:
              c_ids = self.tok_encode(context)
              t_ids = self.tok_encode(continuation)
              ids = (c_ids + t_ids)[-self._max_seq_len:]
              logits = self._forward_logits(ids)
              T_all = logits.size(1)
              T_cont = min(len(t_ids), max(0, T_all - 1))
              logp = 0.0
              greedy = True
              start = T_all - T_cont - 1
              for i in range(T_cont):
                step_idx = start + i
                if step_idx < 0:
                  continue
                probs = torch.log_softmax(logits[0, step_idx], dim=-1)
                tok_id = t_ids[i]
                logp += probs[tok_id].item()
                greedy = greedy and (int(torch.argmax(probs)) == tok_id)
              out.append((float(logp), bool(greedy)))
            return out

          @torch.no_grad()
          def loglikelihood_rolling(self, requests):
            results = []
            for (text,) in requests:
              ids = self.tok_encode(text)
              if len(ids) < 2:
                results.append(0.0)
                continue
              win = self._max_seq_len
              total_lp, i = 0.0, 0
              while i < len(ids) - 1:
                chunk = ids[i:i+win]
                if len(chunk) < 2:
                  break
                logits = self._forward_logits(chunk)
                for k in range(len(chunk) - 1):
                  probs = torch.log_softmax(logits[0, k], dim=-1)
                  total_lp += probs[chunk[k+1]].item()
                i += win - 1
              results.append(total_lp)
            return results

          @torch.no_grad()
          def generate_until(self, requests):
            outputs = []
            for context, until in requests:
              base_ids = self.tok_encode(context)
              ids = base_ids[-(self._max_seq_len-1):]
              text_out = ""
              for _ in range(self._max_new_tokens):
                logits = self._forward_logits(ids)
                next_id = int(torch.argmax(logits[0, -1]).item())
                ids.append(next_id)
                text_out = self.tok_decode(ids[len(base_ids):])
                if any(u in text_out for u in until):
                  break
                if self.eot_token_id is not None and next_id == self.eot_token_id:
                  break
              outputs.append(text_out)
            return outputs

        def main():
          p = argparse.ArgumentParser()
          p.add_argument("--model_py_in", required=True)
          p.add_argument("--model_config", required=True)
          p.add_argument("--model_weights", required=True)
          p.add_argument("--tokenizer_json", required=True)
          p.add_argument("--tasks", required=True)
          p.add_argument("--max_new_tokens", required=True)
          p.add_argument("--batch_size", required=True)
          p.add_argument("--benchmarks_json", required=True)
          args = p.parse_args()

          result = {
            "status": "error",
            "error": None,
            "notes": [],
            "imports": import_errors,
            "versions": versions,
            "emissions_kg": None,
            "tasks_requested": [],
            "tasks_run": [],
            "tasks_skipped": [],
            "lm_eval": {},
          }

          # core libs required
          if any(k in import_errors for k in ("torch", "tokenizers")):
            result["error"] = "Missing core library (torch or tokenizers). See 'imports'."
            safe_write_json(args.benchmarks_json, result)
            return

          # validate files
          for pth in [args.model_py_in, args.model_config, args.model_weights, args.tokenizer_json]:
            if not os.path.exists(pth):
              result["error"] = f"Missing input file: {pth}"
              safe_write_json(args.benchmarks_json, result)
              return

          # ensure importable .py
          model_py_path = args.model_py_in
          if not model_py_path.endswith(".py"):
            tmp_dir = tempfile.mkdtemp(prefix="model_src_")
            tmp_py = os.path.join(tmp_dir, "user_model.py")
            shutil.copyfile(model_py_path, tmp_py)
            model_py_path = tmp_py
            print(f"[INFO] Using importable model file: {model_py_path}")

          # load config
          with open(args.model_config, "r", encoding="utf-8") as f:
            cfg = json.load(f)

          # --- tokenizer load guarded (this is where your crash happened) ---
          try:
            tk_path = args.tokenizer_json
            if os.path.isdir(tk_path):
                # if someone passed a directory, try common names inside it
                for cand in ("tokenizer.json",):
                    c = os.path.join(tk_path, cand)
                    if os.path.exists(c):
                        tk_path = c
                        break
            
            # quick sanity check: file must *look* like a HF tokenizer json
            with open(tk_path, "r", encoding="utf-8", errors="ignore") as _f:
                _head = _f.read(4000)
            if ("\"model\"" not in _head) or ("\"type\"" not in _head):
                raise Exception(f"File at {tk_path} does not appear to be a Hugging Face tokenizer.json")

            tok = Tokenizer.from_file(tk_path)
            
          except Exception as e:
            result["error"] = f"Failed to load tokenizer JSON: {e}"
            result["notes"].append(
              "Likely a version mismatch or a corrupted tokenizer file. "
              "Ensure the tokenizers library version in this brick matches the version used to create the tokenizer."
            )
            safe_write_json(args.benchmarks_json, result)
            return
          # ------------------------------------------------------------------

          device = "cuda" if torch.cuda.is_available() else "cpu"
          max_seq = int(cfg.get("max_seq_len", cfg.get("context_length", 512)))
          max_new = int(args.max_new_tokens)
          bs = int(args.batch_size)

          # import model module
          try:
            spec = importlib.util.spec_from_file_location("user_model", model_py_path)
            if spec is None or spec.loader is None:
              raise RuntimeError(f"Could not load module spec from {model_py_path}")
            mod = importlib.util.module_from_spec(spec)
            sys.modules["user_model"] = mod
            spec.loader.exec_module(mod)
          except Exception as e:
            result["error"] = f"Failed to import model file: {e}"
            safe_write_json(args.benchmarks_json, result)
            return

          # instantiate model
          try:
            ModelClass = getattr(mod, "Gemma3Model", None) or getattr(mod, "TinyGPTNeoX", None)
            if ModelClass is None:
              raise RuntimeError("Neither Gemma3Model nor TinyGPTNeoX found in model file.")
            model = ModelClass(cfg)
            state = torch.load(args.model_weights, map_location="cpu")
            model.load_state_dict(state, strict=False)
            model_dtype = str(cfg.get("dtype", "float32")).lower()
            if device == "cuda" and model_dtype in ("float16", "fp16", "half"):
              model.half()
            model.to(device).eval()
          except Exception as e:
            result["error"] = f"Failed to construct/load model: {e}"
            safe_write_json(args.benchmarks_json, result)
            return

          adapter = GPTNeoXAdapter(
            model=model,
            tokenizer=tok,
            max_seq_len=max_seq,
            device=device,
            batch_size=bs,
            max_new_tokens=max_new,
          )

          requested = [t.strip() for t in args.tasks.split(",") if t.strip()]
          result["tasks_requested"] = requested

          # if lm-eval not available, skip but succeed
          if simple_evaluate is None or TaskManager is None or LM is None:
            result["status"] = "ok"
            result["tasks_skipped"] = requested
            if "lm-eval" in import_errors:
              result["notes"].append(f"lm-eval unavailable: {import_errors.get('lm-eval')}")
            safe_write_json(args.benchmarks_json, result)
            return

          tm = TaskManager(include_path=None) if TaskManager is not None else None
          available, skipped = [], []
          for t in requested:
            try:
              if tm is None:
                raise RuntimeError("TaskManager is unavailable")
              _ = tm.get_task(t)
              available.append(t)
            except Exception:
              skipped.append(t)
          result["tasks_skipped"] = skipped

          tracker = None
          try:
            if EmissionsTracker is not None:
              tracker = EmissionsTracker(log_level="error")
              tracker.start()
            else:
              result["notes"].append("CodeCarbon not available; emissions not recorded.")
          except Exception as e:
            result["notes"].append(f"CodeCarbon tracker not started: {e}")

          # run evaluation
          try:
            eval_dict = simple_evaluate(
              model=adapter,
              tasks=available,
              task_manager=tm,
              num_fewshot=0,
            )
            result["lm_eval"] = eval_dict
            result["tasks_run"] = available
            result["status"] = "ok"
          except Exception as e:
            result["error"] = f"lm-eval failed: {e}"

          try:
            if tracker is not None:
              emissions = tracker.stop()
              result["emissions_kg"] = float(emissions) if emissions is not None else None
          except Exception as e:
            result["notes"].append(f"CodeCarbon stop error: {e}")

          safe_write_json(args.benchmarks_json, result)
          gc.collect()

        if __name__ == "__main__":
          main()
    args:
      - --model_py_in
      - {inputPath: model_py_in}
      - --model_config
      - {inputPath: model_config}
      - --model_weights
      - {inputPath: model_weights}
      - --tokenizer_json
      - {inputPath: tokenizer_json}
      - --tasks
      - {inputValue: tasks}
      - --max_new_tokens
      - {inputValue: max_new_tokens}
      - --batch_size
      - {inputValue: batch_size}
      - --benchmarks_json
      - {outputPath: benchmarks_json}
