name: GPTNeoX
description: Builds an untrained TinyGPTNeoX model from tokenizer vocab and hyperparams, saves initial weights, config, and model source.
inputs:
  - name: tokenizer_json
    type: Model
  - name: num_layers
    type: Integer
    default: "4"
outputs:
  - name: model_weights
    type: Model
  - name: model_config
    type: Data
  - name: model_py
    type: Data
implementation:
  container:
    image: python:3.9
    command:
      - python3
      - -u
      - -c
      - |
        import subprocess, sys, os, json, argparse, importlib.util, textwrap, shutil
        # Install runtime deps (quiet)
        subprocess.run([sys.executable, "-m", "pip", "install", "--quiet", "--no-cache-dir", "torch", "tokenizers", "torchinfo","numpy"], check=True)
        from torchinfo import summary
        import torch
        from tokenizers import Tokenizer

        # === embedded model source (written to model_py at runtime) ===
        MODEL_PY = '''
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

def rotate_half(x):
    x1 = x[..., ::2]
    x2 = x[..., 1::2]
    return torch.stack((-x2, x1), dim=-1).reshape_as(x)

def apply_rotary_pos_emb(q, k, sin, cos):
    q_rot = (q * cos) + (rotate_half(q) * sin)
    k_rot = (k * cos) + (rotate_half(k) * sin)
    return q_rot, k_rot

def make_rotary_sin_cos(seq_len, rotary_dim, device, dtype=torch.float32):
    inv_freq = 1.0 / (10000 ** (torch.arange(0, rotary_dim, 2, device=device, dtype=dtype) / rotary_dim))
    positions = torch.arange(seq_len, device=device, dtype=dtype)
    freqs = torch.einsum("i,j->ij", positions, inv_freq)
    emb = torch.cat((freqs, freqs), dim=-1)
    return emb.sin(), emb.cos()

class MaskedMultiHeadAttention(nn.Module):
    def __init__(self, emb_dim, num_heads, rotary_pct=0.25, dropout=0.0):
        super().__init__()
        assert emb_dim % num_heads == 0
        self.emb_dim = emb_dim
        self.num_heads = num_heads
        self.head_dim = emb_dim // num_heads
        self.rotary_pct = rotary_pct
        self.rotary_dim = int(self.head_dim * rotary_pct)
        if self.rotary_dim % 2 != 0:
            self.rotary_dim -= 1
        self.qkv_proj = nn.Linear(emb_dim, emb_dim * 3, bias=False)
        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sin=None, cos=None, attn_mask=None, past_kv=None):
        B, T, E = x.shape
        qkv = self.qkv_proj(x).view(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        if self.rotary_dim > 0 and sin is not None and cos is not None:
            q_rot, q_pass = q[..., :self.rotary_dim], q[..., self.rotary_dim:]
            k_rot, k_pass = k[..., :self.rotary_dim], k[..., self.rotary_dim:]
            sin = sin.view(1, 1, T, self.rotary_dim)
            cos = cos.view(1, 1, T, self.rotary_dim)
            q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, sin, cos)
            q = torch.cat((q_rot, q_pass), dim=-1)
            k = torch.cat((k_rot, k_pass), dim=-1)
        if past_kv is not None:
            past_k, past_v = past_kv
            k = torch.cat([past_k, k], dim=2)
            v = torch.cat([past_v, v], dim=2)
        present_kv = (k, v)
        scale = 1.0 / math.sqrt(self.head_dim)
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale
        if attn_mask is None:
            q_len, k_len = q.size(2), k.size(2)
            mask = torch.ones((q_len, k_len), device=x.device, dtype=torch.bool).tril()
            attn_mask = mask.unsqueeze(0).unsqueeze(0)
        attn_scores = attn_scores.masked_fill(~attn_mask, float("-inf"))
        attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))
        out = torch.matmul(attn_probs, v).transpose(1, 2).contiguous().view(B, T, E)
        return self.out_proj(out), present_kv

class FeedForward(nn.Module):
    def __init__(self, emb_dim, ff_mult=4, dropout=0.0):
        super().__init__()
        self.fc1 = nn.Linear(emb_dim, emb_dim * ff_mult)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(emb_dim * ff_mult, emb_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.dropout(self.fc2(self.act(self.fc1(x))))

class NeoXBlock(nn.Module):
    def __init__(self, emb_dim, num_heads, rotary_pct=0.25, ff_mult=4, dropout=0.0):
        super().__init__()
        self.ln_attn = nn.LayerNorm(emb_dim)
        self.ln_ff = nn.LayerNorm(emb_dim)
        self.attn = MaskedMultiHeadAttention(emb_dim, num_heads, rotary_pct, dropout)
        self.ff = FeedForward(emb_dim, ff_mult, dropout)

    def forward(self, x, sin=None, cos=None, attn_mask=None, past_kv=None):
        norm_x = self.ln_attn(x)
        attn_out, present_kv = self.attn(norm_x, sin, cos, attn_mask, past_kv)
        x = x + attn_out
        norm_x = self.ln_ff(x)
        ff_out = self.ff(norm_x)
        x = x + ff_out
        return x, present_kv

class TinyGPTNeoX(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        vocab_size, emb_dim = cfg["vocab_size"], cfg["emb_dim"]
        num_layers = cfg.get("num_layers", 4)
        num_heads = cfg.get("num_heads", 8)
        rotary_pct = cfg.get("rotary_pct", 0.25)
        self.emb = nn.Embedding(vocab_size, emb_dim)
        self.pos_max = cfg.get("max_seq_len", 512)
        self.blocks = nn.ModuleList([
            NeoXBlock(emb_dim, num_heads, rotary_pct,
                      ff_mult=cfg.get("ff_mult", 4),
                      dropout=cfg.get("dropout", 0.0))
            for _ in range(num_layers)
        ])
        self.final_ln = nn.LayerNorm(emb_dim)
        self.head = nn.Linear(emb_dim, vocab_size, bias=False)
        self.head.weight = self.emb.weight
        rotary_dim = self.blocks[0].attn.rotary_dim
        if rotary_dim > 0:
            sin, cos = make_rotary_sin_cos(self.pos_max, rotary_dim, device="cpu", dtype=torch.float32)
            self.register_buffer("sin_cache", sin, persistent=False)
            self.register_buffer("cos_cache", cos, persistent=False)
        else:
            self.sin_cache = self.cos_cache = None

    def forward(self, input_ids, past_kvs=None):
        B, T = input_ids.shape
        device = input_ids.device
        x = self.emb(input_ids)
        past_len = past_kvs[0][0].size(2) if past_kvs is not None else 0
        if self.sin_cache is not None:
            sin = self.sin_cache[past_len:past_len + T].to(x.device).to(x.dtype)
            cos = self.cos_cache[past_len:past_len + T].to(x.device).to(x.dtype)
        else:
            sin = cos = None
        new_kvs = []
        for i, block in enumerate(self.blocks):
            past_kv = past_kvs[i] if past_kvs is not None else None
            x, present_kv = block(x, sin=sin, cos=cos, past_kv=past_kv)
            new_kvs.append(present_kv)
        x = self.final_ln(x)
        logits = self.head(x)
        return logits, new_kvs
'''

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--tokenizer_json", required=True)
            ap.add_argument("--num_layers", type=int, required=True)
            ap.add_argument("--model_weights", required=True)
            ap.add_argument("--model_config", required=True)
            ap.add_argument("--model_py", required=True)
            args = ap.parse_args()

            print(f"[INFO] Starting GPTNeoX model creation")
            print(f"[INFO] Tokenizer path: {args.tokenizer_json}")
            print(f"[INFO] Number of layers: {args.num_layers}")

            # Verify tokenizer file exists
            if not os.path.exists(args.tokenizer_json):
                print(f"[ERROR] Tokenizer file not found: {args.tokenizer_json}")
                sys.exit(1)

            # Load tokenizer
            print("[INFO] Loading tokenizer...")
            tok = Tokenizer.from_file(args.tokenizer_json)
            vocab_size = tok.get_vocab_size()
            print(f"[INFO] Vocabulary size: {vocab_size}")

            # Set device and dtype
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model_dtype = torch.float16 if device == "cuda" else torch.float32
            print(f"[INFO] Using device: {device}, dtype: {model_dtype}")

            # Create config
            CONFIG = {
                "vocab_size": vocab_size,
                "emb_dim": 512,
                "num_heads": 8,
                "num_layers": args.num_layers,
                "ff_mult": 4,
                "dropout": 0.0,
                "rotary_pct": 0.25,
                "max_seq_len": 512,
                "dtype": "float16" if model_dtype == torch.float16 else "float32",
            }

            # Save config
            print("[INFO] Saving model config...")
            cfg_to_save = dict(CONFIG)
            output_dir = os.path.dirname(args.model_config)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            with open(args.model_config, "w", encoding="utf-8") as f:
                json.dump(cfg_to_save, f, indent=2)
            print(f"[INFO] Config saved to {args.model_config}")

            # Save model source to the Kubeflow output path
            print("[INFO] Saving model Python source...")
            output_dir = os.path.dirname(args.model_py)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            with open(args.model_py, "w", encoding="utf-8") as f:
                f.write(MODEL_PY)
            print(f"[INFO] Model source saved to {args.model_py}")

            # Create a temporary .py file for importlib (needs .py extension)
            temp_model_file = "/tmp/tinygpt_neox_model.py"
            print(f"[INFO] Creating temporary Python file at {temp_model_file}")
            with open(temp_model_file, "w", encoding="utf-8") as f:
                f.write(MODEL_PY)

            # Verify temp model file was created
            if not os.path.exists(temp_model_file):
                print(f"[ERROR] Temporary model file was not created: {temp_model_file}")
                sys.exit(1)

            # Import the model from the temporary .py file
            print("[INFO] Loading model module...")
            spec = importlib.util.spec_from_file_location("tinygpt_neox_model", temp_model_file)
            if spec is None:
                print(f"[ERROR] Could not create module spec from {temp_model_file}")
                sys.exit(1)
            
            if spec.loader is None:
                print(f"[ERROR] Module spec has no loader for {temp_model_file}")
                sys.exit(1)
            
            mod = importlib.util.module_from_spec(spec)
            sys.modules["tinygpt_neox_model"] = mod
            spec.loader.exec_module(mod)
            print("[INFO] Model module loaded successfully")

            # Create model
            print("[INFO] Initializing model...")
            model = mod.TinyGPTNeoX(CONFIG).to(device)
            if model_dtype == torch.float16:
                model.half()
            model.eval()
            print("[INFO] Model initialized")

            # Print model summary
            print("[INFO] Model summary:")
            vocab_sz = CONFIG["vocab_size"]
            try:
                summary(model, input_data=torch.randint(0, vocab_sz, (1, 32)).to(device))
            except Exception as e:
                print(f"[WARNING] Could not print model summary: {e}")

            # Save model weights
            print("[INFO] Saving model weights...")
            output_dir = os.path.dirname(args.model_weights)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            torch.save(model.state_dict(), args.model_weights)
            print(f"[INFO] Weights saved to {args.model_weights}")

            # Verify weights file exists
            if not os.path.exists(args.model_weights):
                print(f"[ERROR] Weights file was not created: {args.model_weights}")
                sys.exit(1)

            # Calculate and print parameters
            n_params = sum(p.numel() for p in model.parameters())
            num_layers = CONFIG["num_layers"]
            print(f"[INFO] Total parameters: {n_params:,}")
            print(f"[INFO] Number of layers: {num_layers}")
            print("[INFO] Model creation completed successfully!")

            # Clean up temp file
            try:
                os.remove(temp_model_file)
                print(f"[INFO] Cleaned up temporary file: {temp_model_file}")
            except:
                pass

        if __name__ == "__main__":
            main()
    args:
      - --tokenizer_json
      - {inputPath: tokenizer_json}
      - --num_layers
      - {inputValue: num_layers}
      - --model_weights
      - {outputPath: model_weights}
      - --model_config
      - {outputPath: model_config}
      - --model_py
      - {outputPath: model_py}
