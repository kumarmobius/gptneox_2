name: GPTNeoX Model
description: Builds an untrained TinyGPTNeoX model from tokenizer vocab and hyperparams, saves initial weights, config, and model source.
inputs:
  - name: tokenizer_json
    type: Model
  - name: num_layers
    type: Integer
    default: "4"
outputs:
  - name: model_weights
    type: Model
  - name: model_config
    type: Data
  - name: model_py
    type: Data
implementation:
  container:
    image: python:3.9
    command:
      - python3
      - -u
      - -c
      - |
        import subprocess, sys, os, json, argparse, textwrap
        # Install runtime deps (quiet)
        subprocess.run([sys.executable, "-m", "pip", "install", "--quiet", "--no-cache-dir", "torch", "tokenizers", "torchinfo", "numpy"], check=True)
        from torchinfo import summary
        import torch
        from tokenizers import Tokenizer

        # === embedded model source (will be written to model_py at runtime) ===
        MODEL_PY = '''
        import math
        import torch
        import torch.nn as nn
        import torch.nn.functional as F

        def rotate_half(x):
            x1 = x[..., ::2]
            x2 = x[..., 1::2]
            return torch.stack((-x2, x1), dim=-1).reshape_as(x)

        def apply_rotary_pos_emb(q, k, sin, cos):
            q_rot = (q * cos) + (rotate_half(q) * sin)
            k_rot = (k * cos) + (rotate_half(k) * sin)
            return q_rot, k_rot

        def make_rotary_sin_cos(seq_len, rotary_dim, device, dtype=torch.float32):
            inv_freq = 1.0 / (10000 ** (torch.arange(0, rotary_dim, 2, device=device, dtype=dtype) / rotary_dim))
            positions = torch.arange(seq_len, device=device, dtype=dtype)
            freqs = torch.einsum("i,j->ij", positions, inv_freq)
            emb = torch.cat((freqs, freqs), dim=-1)
            return emb.sin(), emb.cos()

        class MaskedMultiHeadAttention(nn.Module):
            def __init__(self, emb_dim, num_heads, rotary_pct=0.25, dropout=0.0):
                super().__init__()
                assert emb_dim % num_heads == 0
                self.emb_dim = emb_dim
                self.num_heads = num_heads
                self.head_dim = emb_dim // num_heads
                self.rotary_pct = rotary_pct
                self.rotary_dim = int(self.head_dim * rotary_pct)
                if self.rotary_dim % 2 != 0:
                    self.rotary_dim -= 1
                self.qkv_proj = nn.Linear(emb_dim, emb_dim * 3, bias=False)
                self.out_proj = nn.Linear(emb_dim, emb_dim, bias=False)
                self.dropout = nn.Dropout(dropout)

            def forward(self, x, sin=None, cos=None, attn_mask=None, past_kv=None):
                B, T, E = x.shape
                qkv = self.qkv_proj(x).view(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
                q, k, v = qkv[0], qkv[1], qkv[2]
                if self.rotary_dim > 0 and sin is not None and cos is not None:
                    q_rot, q_pass = q[..., :self.rotary_dim], q[..., self.rotary_dim:]
                    k_rot, k_pass = k[..., :self.rotary_dim], k[..., self.rotary_dim:]
                    sin = sin.view(1, 1, T, self.rotary_dim)
                    cos = cos.view(1, 1, T, self.rotary_dim)
                    q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, sin, cos)
                    q = torch.cat((q_rot, q_pass), dim=-1)
                    k = torch.cat((k_rot, k_pass), dim=-1)
                if past_kv is not None:
                    past_k, past_v = past_kv
                    k = torch.cat([past_k, k], dim=2)
                    v = torch.cat([past_v, v], dim=2)
                present_kv = (k, v)
                scale = 1.0 / math.sqrt(self.head_dim)
                attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale
                if attn_mask is None:
                    q_len, k_len = q.size(2), k.size(2)
                    mask = torch.ones((q_len, k_len), device=x.device, dtype=torch.bool).tril()
                    attn_mask = mask.unsqueeze(0).unsqueeze(0)
                attn_scores = attn_scores.masked_fill(~attn_mask, float("-inf"))
                attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))
                out = torch.matmul(attn_probs, v).transpose(1, 2).contiguous().view(B, T, E)
                return self.out_proj(out), present_kv

        class FeedForward(nn.Module):
            def __init__(self, emb_dim, ff_mult=4, dropout=0.0):
                super().__init__()
                self.fc1 = nn.Linear(emb_dim, emb_dim * ff_mult)
                self.act = nn.GELU()
                self.fc2 = nn.Linear(emb_dim * ff_mult, emb_dim)
                self.dropout = nn.Dropout(dropout)

            def forward(self, x):
                return self.dropout(self.fc2(self.act(self.fc1(x))))

        class NeoXBlock(nn.Module):
            def __init__(self, emb_dim, num_heads, rotary_pct=0.25, ff_mult=4, dropout=0.0):
                super().__init__()
                self.ln_attn = nn.LayerNorm(emb_dim)
                self.ln_ff = nn.LayerNorm(emb_dim)
                self.attn = MaskedMultiHeadAttention(emb_dim, num_heads, rotary_pct, dropout)
                self.ff = FeedForward(emb_dim, ff_mult, dropout)

            def forward(self, x, sin=None, cos=None, attn_mask=None, past_kv=None):
                norm_x = self.ln_attn(x)
                attn_out, present_kv = self.attn(norm_x, sin, cos, attn_mask, past_kv)
                x = x + attn_out
                norm_x = self.ln_ff(x)
                ff_out = self.ff(norm_x)
                x = x + ff_out
                return x, present_kv

        class TinyGPTNeoX(nn.Module):
            def __init__(self, cfg):
                super().__init__()
                vocab_size = int(cfg.get("vocab_size"))
                emb_dim = int(cfg.get("emb_dim", 256))
                num_layers = int(cfg.get("num_layers", cfg.get("n_layers", 4)))
                num_heads = int(cfg.get("num_heads", cfg.get("n_heads", 8)))
                rotary_pct = float(cfg.get("rotary_pct", 0.25))
                self.emb = nn.Embedding(vocab_size, emb_dim)
                self.pos_max = int(cfg.get("max_seq_len", cfg.get("context_length", 512)))
                self.blocks = nn.ModuleList([
                    NeoXBlock(emb_dim, num_heads, rotary_pct,
                              ff_mult=int(cfg.get("ff_mult", 4)),
                              dropout=float(cfg.get("dropout", 0.0)))
                    for _ in range(num_layers)
                ])
                self.final_ln = nn.LayerNorm(emb_dim)
                self.head = nn.Linear(emb_dim, vocab_size, bias=False)
                self.head.weight = self.emb.weight
                rotary_dim = self.blocks[0].attn.rotary_dim
                if rotary_dim > 0:
                    sin, cos = make_rotary_sin_cos(self.pos_max, rotary_dim, device="cpu", dtype=torch.float32)
                    self.register_buffer("sin_cache", sin, persistent=False)
                    self.register_buffer("cos_cache", cos, persistent=False)
                else:
                    self.sin_cache = self.cos_cache = None

            def forward(self, input_ids, past_kvs=None):
                B, T = input_ids.shape
                device = input_ids.device
                x = self.emb(input_ids)
                past_len = past_kvs[0][0].size(2) if past_kvs is not None else 0
                if self.sin_cache is not None:
                    sin = self.sin_cache[past_len:past_len + T].to(x.device).to(x.dtype)
                    cos = self.cos_cache[past_len:past_len + T].to(x.device).to(x.dtype)
                else:
                    sin = cos = None
                new_kvs = []
                for i, block in enumerate(self.blocks):
                    past_kv = past_kvs[i] if past_kvs is not None else None
                    x, present_kv = block(x, sin=sin, cos=cos, past_kv=past_kv)
                    new_kvs.append(present_kv)
                x = self.final_ln(x)
                logits = self.head(x)
                return logits, new_kvs

        # Thin adapter so trainers expecting Gemma3Model can import it
        class Gemma3Model(TinyGPTNeoX):
            def __init__(self, cfg):
                cfg2 = dict(cfg or {})
                cfg2['num_layers'] = int(cfg2.get('n_layers', cfg2.get('num_layers', 4)))
                cfg2['num_heads']  = int(cfg2.get('n_heads', cfg2.get('num_heads', 8)))
                cfg2['emb_dim']    = int(cfg2.get('emb_dim', cfg2.get('d_model', cfg2.get('emb_dim', 256))))
                cfg2['vocab_size'] = int(cfg2.get('vocab_size', cfg2.get('vocab', cfg2.get('vocab_size', 32000))))
                cfg2['max_seq_len']= int(cfg2.get('context_length', cfg2.get('max_seq_len', 512)))
                if 'ff_mult' not in cfg2:
                    if 'hidden_dim' in cfg2 and cfg2.get('emb_dim'):
                        try:
                            cfg2['ff_mult'] = max(1, int(cfg2['hidden_dim'] // cfg2['emb_dim']))
                        except Exception:
                            cfg2['ff_mult'] = 4
                    else:
                        cfg2['ff_mult'] = int(cfg2.get('ff_mult', 4))
                cfg2.setdefault('dropout', float(cfg2.get('dropout', 0.0)))
                super().__init__(cfg2)
        '''

        def safe_make_dirs(path):
            if path:
                os.makedirs(path, exist_ok=True)

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--tokenizer_json", required=True)
            ap.add_argument("--num_layers", type=int, required=True)
            ap.add_argument("--model_weights", required=True)
            ap.add_argument("--model_config", required=True)
            ap.add_argument("--model_py", required=True)
            args = ap.parse_args()

            # --- write model_py artifact (so KFP can capture it) ---
            try:
                safe_make_dirs(os.path.dirname(args.model_py) or ".")
                with open(args.model_py, "w", encoding="utf-8") as f:
                    f.write(MODEL_PY)
                print(f"[INFO] Model code saved to {args.model_py}")
            except Exception as e:
                print(f"[ERROR] Failed to write model_py to {args.model_py}: {e}")
                sys.exit(1)

            # --- load model definitions into this process (avoid importlib issues) ---
            try:
                exec(MODEL_PY, globals())
                print("[INFO] Executed MODEL_PY into current namespace")
            except Exception as e:
                print(f"[ERROR] exec(MODEL_PY) failed: {e}")
                sys.exit(1)

            # --- build CONFIG from tokenizer ---
            try:
                tok = Tokenizer.from_file(args.tokenizer_json)
                vocab_size = tok.get_vocab_size()
            except Exception as e:
                print(f"[ERROR] Failed to load tokenizer: {e}")
                sys.exit(1)

            CONFIG = {
                "vocab_size": int(vocab_size),
                "emb_dim": 256,
                "num_layers": args.num_layers,
                "num_heads": 8,
                "max_seq_len": 512,
                "ff_mult": 4,
                "dropout": 0.0,
            }

            # device + dtype selection
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model_dtype = torch.float16 if device == "cuda" else torch.float32

            # Initialize model (TinyGPTNeoX should be defined by exec above)
            try:
                model = globals()["TinyGPTNeoX"](CONFIG).to(device)
            except Exception as e:
                print(f"[ERROR] Failed to initialize TinyGPTNeoX: {e}")
                sys.exit(1)

            if model_dtype == torch.float16:
                model.half()
            model.eval()
            print("[INFO] Model initialized")

            # Model summary (best-effort)
            try:
                summary(model, input_data=torch.randint(0, CONFIG["vocab_size"], (1, 32)).to(device))
            except Exception as e:
                print(f"[WARN] Could not run torchinfo.summary: {e}")

            # ------------------------------
            # Save weights (user code 1)
            # ------------------------------
            try:
                safe_make_dirs(os.path.dirname(args.model_weights) or ".")
                torch.save(model.state_dict(), args.model_weights)
                print(f"[INFO] Weights saved to {args.model_weights}")
            except Exception as e:
                print(f"[ERROR] Failed to save weights to {args.model_weights}: {e}")
                sys.exit(1)

            # ------------------------------
            # Save config (robust for brick2 expectations)
            # ------------------------------
            try:
                # dtype name for trainer compatibility
                dtype_name = "float16" if model_dtype == torch.float16 else "float32"

                # Start from CONFIG and normalize/ensure keys brick2 expects
                cfg_to_save = dict(CONFIG) if isinstance(CONFIG, dict) else {}
                cfg_to_save["dtype"] = dtype_name

                # context_length fallback chain: explicit -> max_seq_len -> context -> 512
                ctx = cfg_to_save.get("context_length",
                                      cfg_to_save.get("max_seq_len",
                                                      cfg_to_save.get("context", 512)))
                cfg_to_save["context_length"] = int(ctx)
                # ensure max_seq_len is present too
                cfg_to_save["max_seq_len"] = int(cfg_to_save.get("max_seq_len", cfg_to_save["context_length"]))

                # sliding_window default used by brick2
                cfg_to_save["sliding_window"] = int(cfg_to_save.get("sliding_window",
                                            min(512, cfg_to_save["max_seq_len"])))

                # determine num_layers value (respect n_layers if present)
                default_num_layers = int(getattr(args, "num_layers", 4) or 4)
                num_layers_val = int(cfg_to_save.get("num_layers", cfg_to_save.get("n_layers", default_num_layers)))
                cfg_to_save["num_layers"] = num_layers_val

                # layer_types default
                cfg_to_save["layer_types"] = cfg_to_save.get("layer_types", ["full_attention"] * num_layers_val)

                # rotary/rope/gating defaults
                cfg_to_save["rope_base"] = float(cfg_to_save.get("rope_base", 1000000.0))
                cfg_to_save["rope_local_base"] = float(cfg_to_save.get("rope_local_base", 10000.0))
                cfg_to_save["n_kv_groups"] = int(cfg_to_save.get("n_kv_groups", 1))
                cfg_to_save["qk_norm"] = bool(cfg_to_save.get("qk_norm", True))

                # core architecture fields (ensure ints)
                cfg_to_save["vocab_size"] = int(cfg_to_save.get("vocab_size", cfg_to_save.get("vocab", vocab_size)))
                cfg_to_save["emb_dim"] = int(cfg_to_save.get("emb_dim", cfg_to_save.get("d_model", 256)))
                cfg_to_save["num_heads"] = int(cfg_to_save.get("num_heads", cfg_to_save.get("n_heads", 8)))
                cfg_to_save["head_dim"] = int(cfg_to_save.get("head_dim", max(1, cfg_to_save["emb_dim"] // cfg_to_save["num_heads"])))
                cfg_to_save["hidden_dim"] = int(cfg_to_save.get("hidden_dim", cfg_to_save["emb_dim"] * int(cfg_to_save.get("ff_mult", 4))))
                cfg_to_save["ff_mult"] = int(cfg_to_save.get("ff_mult", max(1, cfg_to_save["hidden_dim"] // max(1, cfg_to_save["emb_dim"]))))

                # add param counts for debugging
                try:
                    total_params = int(sum(int(p.numel()) for p in model.parameters()))
                    trainable_params = int(sum(int(p.numel()) for p in model.parameters() if p.requires_grad))
                except Exception:
                    total_params = 0
                    trainable_params = 0
                cfg_to_save["total_parameters"] = total_params
                cfg_to_save["trainable_parameters"] = trainable_params

                # Debug prints (safe: use .get so missing keys don't raise)
                print(f"[DEBUG] Writing model_config with keys: {sorted(list(cfg_to_save.keys()))}")
                print("[DEBUG] " + " ".join(f"{k}={repr(cfg_to_save.get(k))}" for k in
                      ("context_length", "sliding_window", "vocab_size", "emb_dim", "num_layers", "num_heads", "dtype", "total_parameters", "trainable_parameters")))
                print(f"[DEBUG] params: total={total_params:,}, trainable={trainable_params:,}")

                # Write JSON config
                safe_make_dirs(os.path.dirname(args.model_config) or ".")
                with open(args.model_config, "w", encoding="utf-8") as f:
                    json.dump(cfg_to_save, f, indent=2)

                print(f"[INFO] Model config saved to {args.model_config}")
            except Exception as e:
                print(f"[ERROR] Failed to save model config to {args.model_config}: {e}")
                sys.exit(1)

            # Final info
            try:
                n_params = sum(p.numel() for p in model.parameters())
            except Exception:
                n_params = 0
            print(f"[INFO] Total parameters: {n_params:,}")
            print("[INFO] Model creation completed successfully!")

        if __name__ == "__main__":
            main()
    args:
      - --tokenizer_json
      - {inputPath: tokenizer_json}
      - --num_layers
      - {inputValue: num_layers}
      - --model_weights
      - {outputPath: model_weights}
      - --model_config
      - {outputPath: model_config}
      - --model_py
      - {outputPath: model_py}
