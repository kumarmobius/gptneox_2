name: GPTNeoX Model
description: Builds an untrained TinyGPTNeoX model from tokenizer vocab and hyperparams, saves initial weights, config, and model source.
inputs:
  - name: tokenizer_json
    type: Model
  - name: num_layers
    type: Integer
    default: "4"
outputs:
  - name: model_weights
    type: Model
  - name: model_config
    type: Data
  - name: model_py
    type: Data
implementation:
  container:
    image: python:3.9
    command:
      - python3
      - -u
      - -c
      - |
        import subprocess, sys, os, json, argparse, importlib.util, textwrap
        subprocess.run([sys.executable, "-m", "pip", "install", "--quiet", "--no-cache-dir", "torch", "tokenizers", "torchinfo"], check=True)
        from torchinfo import summary
        import torch
        from tokenizers import Tokenizer
        import textwrap

        MODEL_PY = (
            "import math\n"
            "import torch\n"
            "import torch.nn as nn\n"
            "import torch.nn.functional as F\n\n"
            "def rotate_half(x):\n"
            "    x1 = x[..., ::2]\n"
            "    x2 = x[..., 1::2]\n"
            "    return torch.stack((-x2, x1), dim=-1).reshape_as(x)\n\n"
            "def apply_rotary_pos_emb(q, k, sin, cos):\n"
            "    q_rot = (q * cos) + (rotate_half(q) * sin)\n"
            "    k_rot = (k * cos) + (rotate_half(k) * sin)\n"
            "    return q_rot, k_rot\n\n"
            "def make_rotary_sin_cos(seq_len, rotary_dim, device, dtype=torch.float32):\n"
            "    inv_freq = 1.0 / (10000 ** (torch.arange(0, rotary_dim, 2, device=device, dtype=dtype) / rotary_dim))\n"
            "    positions = torch.arange(seq_len, device=device, dtype=dtype)\n"
            "    freqs = torch.einsum('i,j->ij', positions, inv_freq)\n"
            "    emb = torch.cat((freqs, freqs), dim=-1)\n"
            "    return emb.sin(), emb.cos()\n\n"
            "class MaskedMultiHeadAttention(nn.Module):\n"
            "    def __init__(self, emb_dim, num_heads, rotary_pct=0.25, dropout=0.0):\n"
            "        super().__init__()\n"
            "        assert emb_dim % num_heads == 0\n"
            "        self.emb_dim = emb_dim\n"
            "        self.num_heads = num_heads\n"
            "        self.head_dim = emb_dim // num_heads\n"
            "        self.rotary_pct = rotary_pct\n"
            "        self.rotary_dim = int(self.head_dim * rotary_pct)\n"
            "        if self.rotary_dim % 2 != 0:\n"
            "            self.rotary_dim -= 1\n"
            "        self.qkv_proj = nn.Linear(emb_dim, emb_dim * 3, bias=False)\n"
            "        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n"
            "        self.dropout = nn.Dropout(dropout)\n\n"
            "    def forward(self, x, sin=None, cos=None, attn_mask=None, past_kv=None):\n"
            "        B, T, E = x.shape\n"
            "        qkv = self.qkv_proj(x).view(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n"
            "        q, k, v = qkv[0], qkv[1], qkv[2]\n"
            "        if self.rotary_dim > 0 and sin is not None and cos is not None:\n"
            "            q_rot, q_pass = q[..., :self.rotary_dim], q[..., self.rotary_dim:]\n"
            "            k_rot, k_pass = k[..., :self.rotary_dim], k[..., self.rotary_dim:]\n"
            "            sin = sin.view(1, 1, T, self.rotary_dim)\n"
            "            cos = cos.view(1, 1, T, self.rotary_dim)\n"
            "            q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, sin, cos)\n"
            "            q = torch.cat((q_rot, q_pass), dim=-1)\n"
            "            k = torch.cat((k_rot, k_pass), dim=-1)\n"
            "        if past_kv is not None:\n"
            "            past_k, past_v = past_kv\n"
            "            k = torch.cat([past_k, k], dim=2)\n"
            "            v = torch.cat([past_v, v], dim=2)\n"
            "        present_kv = (k, v)\n"
            "        scale = 1.0 / math.sqrt(self.head_dim)\n"
            "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n"
            "        if attn_mask is None:\n"
            "            q_len, k_len = q.size(2), k.size(2)\n"
            "            mask = torch.ones((q_len, k_len), device=x.device, dtype=torch.bool).tril()\n"
            "            attn_mask = mask.unsqueeze(0).unsqueeze(0)\n"
            "        attn_scores = attn_scores.masked_fill(~attn_mask, float('-inf'))\n"
            "        attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))\n"
            "        out = torch.matmul(attn_probs, v).transpose(1, 2).contiguous().view(B, T, E)\n"
            "        return self.out_proj(out), present_kv\n\n"
            "class FeedForward(nn.Module):\n"
            "    def __init__(self, emb_dim, ff_mult=4, dropout=0.0):\n"
            "        super().__init__()\n"
            "        self.fc1 = nn.Linear(emb_dim, emb_dim * ff_mult)\n"
            "        self.act = nn.GELU()\n"
            "        self.fc2 = nn.Linear(emb_dim * ff_mult, emb_dim)\n"
            "        self.dropout = nn.Dropout(dropout)\n\n"
            "    def forward(self, x):\n"
            "        return self.dropout(self.fc2(self.act(self.fc1(x))))\n\n"
            "class NeoXBlock(nn.Module):\n"
            "    def __init__(self, emb_dim, num_heads, rotary_pct=0.25, ff_mult=4, dropout=0.0):\n"
            "        super().__init__()\n"
            "        self.ln_attn = nn.LayerNorm(emb_dim)\n"
            "        self.ln_ff = nn.LayerNorm(emb_dim)\n"
            "        self.attn = MaskedMultiHeadAttention(emb_dim, num_heads, rotary_pct, dropout)\n"
            "        self.ff = FeedForward(emb_dim, ff_mult, dropout)\n\n"
            "    def forward(self, x, sin=None, cos=None, attn_mask=None, past_kv=None):\n"
            "        norm_x = self.ln_attn(x)\n"
            "        attn_out, present_kv = self.attn(norm_x, sin, cos, attn_mask, past_kv)\n"
            "        x = x + attn_out\n"
            "        norm_x = self.ln_ff(x)\n"
            "        ff_out = self.ff(norm_x)\n"
            "        x = x + ff_out\n"
            "        return x, present_kv\n\n"
            "class TinyGPTNeoX(nn.Module):\n"
            "    def __init__(self, cfg):\n"
            "        super().__init__()\n"
            "        vocab_size, emb_dim = cfg['vocab_size'], cfg['emb_dim']\n"
            "        num_layers = cfg.get('num_layers', 4)\n"
            "        num_heads = cfg.get('num_heads', 8)\n"
            "        rotary_pct = cfg.get('rotary_pct', 0.25)\n"
            "        self.emb = nn.Embedding(vocab_size, emb_dim)\n"
            "        self.pos_max = cfg.get('max_seq_len', 512)\n"
            "        self.blocks = nn.ModuleList([\n"
            "            NeoXBlock(emb_dim, num_heads, rotary_pct,\n"
            "                      ff_mult=cfg.get('ff_mult', 4),\n"
            "                      dropout=cfg.get('dropout', 0.0))\n"
            "            for _ in range(num_layers)\n"
            "        ])\n"
            "        self.final_ln = nn.LayerNorm(emb_dim)\n"
            "        self.head = nn.Linear(emb_dim, vocab_size, bias=False)\n"
            "        self.head.weight = self.emb.weight\n"
            "        rotary_dim = self.blocks[0].attn.rotary_dim\n"
            "        if rotary_dim > 0:\n"
            "            sin, cos = make_rotary_sin_cos(self.pos_max, rotary_dim, device='cpu', dtype=torch.float32)\n"
            "            self.register_buffer('sin_cache', sin, persistent=False)\n"
            "            self.register_buffer('cos_cache', cos, persistent=False)\n"
            "        else:\n"
            "            self.sin_cache = self.cos_cache = None\n\n"
            "    def forward(self, input_ids, past_kvs=None):\n"
            "        B, T = input_ids.shape\n"
            "        device = input_ids.device\n"
            "        x = self.emb(input_ids)\n"
            "        past_len = past_kvs[0][0].size(2) if past_kvs is not None else 0\n"
            "        if self.sin_cache is not None:\n"
            "            sin = self.sin_cache[past_len:past_len + T].to(x.device).to(x.dtype)\n"
            "            cos = self.cos_cache[past_len:past_len + T].to(x.device).to(x.dtype)\n"
            "        else:\n"
            "            sin = cos = None\n"
            "        new_kvs = []\n"
            "        for i, block in enumerate(self.blocks):\n"
            "            past_kv = past_kvs[i] if past_kvs is not None else None\n"
            "            x, present_kv = block(x, sin=sin, cos=cos, past_kv=past_kv)\n"
            "            new_kvs.append(present_kv)\n"
            "        x = self.final_ln(x)\n"
            "        logits = self.head(x)\n"
            "        return logits, new_kvs\n"
        )

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--tokenizer_json", required=True)
            ap.add_argument("--num_layers", type=int, required=True)
            ap.add_argument("--model_weights", required=True)
            ap.add_argument("--model_config", required=True)
            ap.add_argument("--model_py", required=True)
            args = ap.parse_args()

            if not os.path.exists(args.tokenizer_json):
                print("[ERROR] Tokenizer file not found: " + args.tokenizer_json)
                sys.exit(1)

            tok = Tokenizer.from_file(args.tokenizer_json)
            vocab_size = tok.get_vocab_size()

            device = "cuda" if torch.cuda.is_available() else "cpu"
            model_dtype = torch.float16 if device == "cuda" else torch.float32

            CONFIG = {
                "vocab_size": vocab_size,
                "emb_dim": 512,
                "num_heads": 8,
                "num_layers": args.num_layers,
                "ff_mult": 4,
                "dropout": 0.0,
                "rotary_pct": 0.25,
                "max_seq_len": 512,
                "dtype": "float16" if model_dtype == torch.float16 else "float32",
            }

            cfg_to_save = dict(CONFIG)
            os.makedirs(os.path.dirname(args.model_config) or ".", exist_ok=True)
            with open(args.model_config, "w", encoding="utf-8") as f:
                json.dump(cfg_to_save, f, indent=2)

            os.makedirs(os.path.dirname(args.model_py) or ".", exist_ok=True)
            with open(args.model_py, "w", encoding="utf-8") as f:
                f.write(MODEL_PY)

            spec = importlib.util.spec_from_file_location("tinygpt_neox_model", args.model_py)
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)

            model = mod.TinyGPTNeoX(CONFIG).to(device)
            if model_dtype == torch.float16:
                model.half()
            model.to(device).eval()

            vocab_sz = CONFIG["vocab_size"]
            summary(model, input_data=torch.randint(0, vocab_sz, (1, 32)).to(device))

            os.makedirs(os.path.dirname(args.model_weights) or ".", exist_ok=True)
            torch.save(model.state_dict(), args.model_weights)

        if __name__ == "__main__":
            main()
    args:
      - --tokenizer_json
      - {inputPath: tokenizer_json}
      - --num_layers
      - {inputValue: num_layers}
      - --model_weights
      - {outputPath: model_weights}
      - --model_config
      - {outputPath: model_config}
      - --model_py
      - {outputPath: model_py}
